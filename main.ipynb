{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Research--- Sentiment Analysis of Different Models on Different Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re,string, random\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 20)\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random as rn\n",
    "from text_processor import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Test Models on Twitter Sentiment 140 Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data of Sentiment 140 sample from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in accessing the database\n"
     ]
    }
   ],
   "source": [
    "remote_db=\"localhost\"\n",
    "remote_link = \"mongodb://\" + remote_db + \":27017/\"\n",
    "my_client = pymongo.MongoClient(remote_link)\n",
    "try:\n",
    "    info = my_client.server_info()  # Forces a call.\n",
    "    my_db=my_client.get_database('Twitter_Sentiment140')\n",
    "    my_tb=my_db['Sentiment140']\n",
    "    np.random.seed(10)  #use seed, to make sure the result is replicable\n",
    "    random_test_set_seq=np.random.choice(range(1,100001), 20000, replace=False).tolist()\n",
    "    raw_test_data=my_tb.find({\"sequence_no\": {'$in': random_test_set_seq}})\n",
    "    raw_train_data=my_tb.find({\"sequence_no\": {'$nin': random_test_set_seq}})\n",
    "    print(\"Success in accessing the database\")\n",
    "except ServerSelectionTimeoutError:\n",
    "    print(\"Database is down.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using lists/matrixes to save all train data and test data (preparing for models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=[]\n",
    "train_token_list_matrix=[]\n",
    "test_label=[]\n",
    "test_token_list_matrix=[]\n",
    "train_raw_text=[]\n",
    "test_raw_text=[]\n",
    "for each_row in raw_train_data:\n",
    "    train_label.append(each_row['label'])\n",
    "    train_token_list_matrix.append(each_row['tokenized_text'])\n",
    "    train_raw_text.append(each_row['text'])\n",
    "for each_row in raw_test_data:\n",
    "    test_label.append(each_row['label'])\n",
    "    test_token_list_matrix.append(each_row['tokenized_text'])\n",
    "    test_raw_text.append(each_row['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Naive Bayes Model,the input training data and test data have to be further processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "train_dataset_for_NB=[]\n",
    "test_dataset_for_NB=[]\n",
    "for each_token_list in train_token_list_matrix:\n",
    "    tweet_dict=dict([token, True] for token in each_token_list)\n",
    "    if(train_label[index]==-1):\n",
    "        train_dataset_for_NB.append((tweet_dict,\"Negative\"))\n",
    "    else:\n",
    "        train_dataset_for_NB.append((tweet_dict, \"Positive\"))\n",
    "    index=index+1\n",
    "for each_token_list in test_token_list_matrix:\n",
    "    tweet_dict=dict([token, True] for token in each_token_list)\n",
    "    test_dataset_for_NB.append(tweet_dict)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_classifier = NaiveBayesClassifier.train(train_dataset_for_NB[:80000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of NB_classifier on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.74925\n",
      "Confusion Matrix is:  [[8378 1634]\n",
      " [3381 6607]]\n"
     ]
    }
   ],
   "source": [
    "NB_result=[]\n",
    "for each_tweet in test_dataset_for_NB:\n",
    "    if NB_classifier.classify(each_tweet)=='Negative':\n",
    "        NB_result.append(-1)\n",
    "    else:\n",
    "        NB_result.append(1)\n",
    "print(\"The accuracy score is: \",accuracy_score(test_label,NB_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(test_label,NB_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of NB_classifier on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.8395375\n",
      "Confusion Matrix is:  [[36249  3589]\n",
      " [ 9248 30914]]\n"
     ]
    }
   ],
   "source": [
    "NB_result=[]\n",
    "for each_tweet in train_dataset_for_NB[:80000]:\n",
    "    if NB_classifier.classify(each_tweet[0])=='Negative':\n",
    "        NB_result.append(-1)\n",
    "    else:\n",
    "        NB_result.append(1)\n",
    "print(\"The accuracy score is: \",accuracy_score(train_label[:80000],NB_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(train_label[:80000],NB_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Test Vader's performance on test dataset. Notice Vader is rule-based;it has no training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.51845\n",
      "Confusion Matrix is:  [[4227 2636 3149]\n",
      " [   0    0    0]\n",
      " [1003 2843 6142]]\n"
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "vader_result=[]\n",
    "for each_tweet in test_raw_text:\n",
    "    score=sia.polarity_scores(each_tweet)['compound']\n",
    "    if(score<=-0.05):\n",
    "        vader_result.append(-1)\n",
    "    elif(score>0.05):\n",
    "        vader_result.append(1)\n",
    "    else:\n",
    "        vader_result.append(0)\n",
    "print(\"The accuracy score is: \",accuracy_score(test_label,vader_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(test_label,vader_result,labels=[-1,0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "doc2vec_model= Doc2Vec.load(\"Doc2Vec/doc2vec_model.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM,RF and LSTM,the training data and test data have to be further processed with Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_vectorize=[]\n",
    "test_dataset_vectorize=[]\n",
    "for each_token_list in train_token_list_matrix:\n",
    "    vector = doc2vec_model.infer_vector(each_token_list)\n",
    "    train_dataset_vectorize.append(vector)\n",
    "for each_token_list in test_token_list_matrix:\n",
    "    vector = doc2vec_model.infer_vector(each_token_list)\n",
    "    test_dataset_vectorize.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model=SVC(C=1, kernel='rbf',random_state=10)\n",
    "svm_model.fit(train_dataset_vectorize[:80000],train_label[:80000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure SVM's performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.69365\n",
      "Confusion Matrix is:  [[6614 3398]\n",
      " [2729 7259]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=svm_model.predict(test_dataset_vectorize)\n",
    "print(\"The accuracy score is: \",accuracy_score(test_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(test_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure SVM's performance on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.7943125\n",
      "Confusion Matrix is:  [[30262  9576]\n",
      " [ 6879 33283]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=svm_model.predict(train_dataset_vectorize[:80000])\n",
    "print(\"The accuracy score is: \",accuracy_score(train_label[:80000],predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(train_label[:80000],predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=10, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
       "                       n_jobs=-1, oob_score=False, random_state=10, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model=RandomForestClassifier(n_estimators=400,criterion='gini',max_depth=10,random_state=10,n_jobs=-1)\n",
    "rf_model.fit(train_dataset_vectorize[:80000],train_label[:80000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure RF's performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.65325\n",
      "Confusion Matrix is:  [[6267 3745]\n",
      " [3190 6798]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=rf_model.predict(test_dataset_vectorize)\n",
    "print(\"The accuracy score is: \",accuracy_score(test_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(test_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure RF's performance on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.8376375\n",
      "Confusion Matrix is:  [[31851  7987]\n",
      " [ 5002 35160]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=rf_model.predict(train_dataset_vectorize[:80000])\n",
    "print(\"The accuracy score is: \",accuracy_score(train_label[:80000],predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(train_label[:80000],predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce Neural Network (Keras+Tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#comment/uncomment to choose CPU or GPU\n",
    "#choose CPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" \n",
    "# np.random.seed(10)\n",
    "# rn.seed(10)\n",
    "# os.environ['PYTHONHASHSEED']=str(10)\n",
    "# config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=12,inter_op_parallelism_threads=12,device_count = {'CPU':12})\n",
    "# tf.random.set_seed(10)\n",
    "# sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "# tf.compat.v1.keras.backend.set_session(sess)\n",
    "#choose GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # set the value to 0, the system will use the first GPU detected\n",
    "# ---------------------------------------\n",
    "#notice while GPU is used, we can not use random seed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(hidden_layer, hidden_unit, opt):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_unit, input_dim=200, activation='relu'))  #input layer+first hidden layer\n",
    "    for i in range(hidden_layer - 1):  # the line above already adds one hidden layer\n",
    "        model.add(Dense(hidden_unit, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  #output layer\n",
    "    model.compile(loss='mean_squared_logarithmic_error', optimizer=opt, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80000/80000 [==============================] - 6s 75us/step - loss: 0.1003 - accuracy: 0.6521\n",
      "Epoch 2/10\n",
      "80000/80000 [==============================] - 6s 73us/step - loss: 0.0949 - accuracy: 0.6794\n",
      "Epoch 3/10\n",
      "80000/80000 [==============================] - 6s 72us/step - loss: 0.0907 - accuracy: 0.6999\n",
      "Epoch 4/10\n",
      "80000/80000 [==============================] - 6s 73us/step - loss: 0.0866 - accuracy: 0.7177\n",
      "Epoch 5/10\n",
      "80000/80000 [==============================] - 6s 72us/step - loss: 0.0818 - accuracy: 0.7393\n",
      "Epoch 6/10\n",
      "80000/80000 [==============================] - 6s 72us/step - loss: 0.0765 - accuracy: 0.7604\n",
      "Epoch 7/10\n",
      "80000/80000 [==============================] - 6s 72us/step - loss: 0.0713 - accuracy: 0.7804\n",
      "Epoch 8/10\n",
      "80000/80000 [==============================] - 6s 74us/step - loss: 0.0665 - accuracy: 0.7969\n",
      "Epoch 9/10\n",
      "80000/80000 [==============================] - 6s 75us/step - loss: 0.0619 - accuracy: 0.8149\n",
      "Epoch 10/10\n",
      "80000/80000 [==============================] - 5s 69us/step - loss: 0.0580 - accuracy: 0.8273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1aac0e54fd0>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = KerasClassifier(build_fn=neural_network_model, hidden_layer=3,\n",
    "                                        hidden_unit=256, opt='adam', epochs=10,\n",
    "                                        batch_size=32)\n",
    "nn_model.fit(np.array(train_dataset_vectorize[:80000]),np.array(train_label[:80000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure Neural Network's performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.67655\n",
      "Confusion Matrix is:  [[6460 3552]\n",
      " [2917 7071]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result = nn_model.predict(np.array(test_dataset_vectorize))\n",
    "predicted_result = predicted_result.reshape((1, len(test_dataset_vectorize)))[0]\n",
    "print(\"The accuracy score is: \",accuracy_score(test_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(test_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure Neural Network's performance on test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.8483125\n",
      "Confusion Matrix is:  [[32829  7009]\n",
      " [ 5126 35036]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=nn_model.predict(np.array(train_dataset_vectorize[:80000]))\n",
    "predicted_result = predicted_result.reshape((1, 80000))[0]\n",
    "print(\"The accuracy score is: \",accuracy_score(train_label[:80000],predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(train_label[:80000],predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Test on IMDb Movie Reviews Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get IMDb movie reviews sample from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in accessing the database\n"
     ]
    }
   ],
   "source": [
    "remote_db=\"localhost\"\n",
    "remote_link = \"mongodb://\" + remote_db + \":27017/\"\n",
    "my_client = pymongo.MongoClient(remote_link)\n",
    "try:\n",
    "    info = my_client.server_info()  # Forces a call.\n",
    "    my_db=my_client.get_database('IMDb')\n",
    "    my_tb=my_db['movie_reviews']\n",
    "    np.random.seed(10)  #use seed, to make sure the result is replicable\n",
    "    random_train_set_seq=np.random.choice(range(1,9906), 7920, replace=False).tolist()  #take 7920 movie reviews to train (about 80%), at random\n",
    "    imdb_raw_test_data=my_tb.find({\"sequence_no\": {'$nin': random_train_set_seq}})\n",
    "    imdb_raw_train_data=my_tb.find({\"sequence_no\": {'$in': random_train_set_seq}})\n",
    "    print(\"Success in accessing the database\")\n",
    "except ServerSelectionTimeoutError:\n",
    "    print(\"Database is down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_label=[]\n",
    "imdb_train_token_list_matrix=[]\n",
    "imdb_test_label=[]\n",
    "imdb_test_token_list_matrix=[]\n",
    "imdb_train_raw_text=[]\n",
    "imdb_test_raw_text=[]\n",
    "for each_row in imdb_raw_train_data:\n",
    "    if each_row['scores']<=6:   #the mean score of IMDb movie reviews is between 6-7, according to Google；so score below 6 is thought as negative\n",
    "        imdb_train_label.append(-1)\n",
    "    else:\n",
    "        imdb_train_label.append(1)\n",
    "    imdb_raw_text=each_row['review_titles']+\" \"+each_row['comment']  #title+comment\n",
    "    imdb_train_token_list_matrix.append(processText_for_sentiment_analysis(imdb_raw_text))\n",
    "    imdb_train_raw_text.append(imdb_raw_text)\n",
    "for each_row in imdb_raw_test_data:\n",
    "    if each_row['scores']<=6:   #the mean score of IMDb movie reviews is between 6-7, according to Google\n",
    "        imdb_test_label.append(-1)\n",
    "    else:\n",
    "        imdb_test_label.append(1)\n",
    "    imdb_raw_text=each_row['review_titles']+\" \"+each_row['comment']  #title+comment\n",
    "    imdb_test_token_list_matrix.append(processText_for_sentiment_analysis(imdb_raw_text))\n",
    "    imdb_test_raw_text.append(imdb_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data distribution:\n",
      "In training dataset, there are 4714 positive reviews and 3206 negative reviews\n",
      "In test dataset, there are 1186 positive reviews and 799 negative reviews\n"
     ]
    }
   ],
   "source": [
    "print(\"Data distribution:\")\n",
    "count_pos=0\n",
    "count_neg=0\n",
    "for i in imdb_train_label:\n",
    "    if(i==1):\n",
    "        count_pos=count_pos+1\n",
    "    else:\n",
    "        count_neg=count_neg+1\n",
    "print(\"In training dataset, there are {} positive reviews and {} negative reviews\".format(count_pos,count_neg))\n",
    "count_pos=0\n",
    "count_neg=0\n",
    "for i in imdb_test_label:\n",
    "    if(i==1):\n",
    "        count_pos=count_pos+1\n",
    "    else:\n",
    "        count_neg=count_neg+1\n",
    "print(\"In test dataset, there are {} positive reviews and {} negative reviews\".format(count_pos,count_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "imdb_train_dataset_for_NB=[]\n",
    "imdb_test_dataset_for_NB=[]\n",
    "for each_token_list in imdb_train_token_list_matrix:\n",
    "    tweet_dict=dict([token, True] for token in each_token_list)\n",
    "    if(imdb_train_label[index]==-1):\n",
    "        imdb_train_dataset_for_NB.append((tweet_dict,\"Negative\"))\n",
    "    else:\n",
    "        imdb_train_dataset_for_NB.append((tweet_dict, \"Positive\"))\n",
    "    index=index+1\n",
    "\n",
    "for each_token_list in imdb_test_token_list_matrix:\n",
    "    tweet_dict=dict([token, True] for token in each_token_list)\n",
    "    imdb_test_dataset_for_NB.append(tweet_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb_NB_classifier = NaiveBayesClassifier.train(imdb_train_dataset_for_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of Naive Bayes on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.8473551637279597\n",
      "The f1 score is:  0.8789452656811826\n",
      "Confusion Matrix is:  [[ 582  217]\n",
      " [  86 1100]]\n"
     ]
    }
   ],
   "source": [
    "imdb_NB_result=[]\n",
    "for each_comment in imdb_test_dataset_for_NB:\n",
    "    if IMDb_NB_classifier.classify(each_comment)=='Negative':\n",
    "        imdb_NB_result.append(-1)\n",
    "    else:\n",
    "        imdb_NB_result.append(1)\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_test_label,imdb_NB_result))\n",
    "print(\"The f1 score is: \",f1_score(imdb_test_label,imdb_NB_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_test_label,imdb_NB_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of Naive Bayes on Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.9142676767676767\n",
      "The f1 score is:  0.9293370798210011\n",
      "Confusion Matrix is:  [[2776  430]\n",
      " [ 249 4465]]\n"
     ]
    }
   ],
   "source": [
    "imdb_NB_result=[]\n",
    "for each_comment in imdb_train_dataset_for_NB:\n",
    "    if IMDb_NB_classifier.classify(each_comment[0])=='Negative':\n",
    "        imdb_NB_result.append(-1)\n",
    "    else:\n",
    "        imdb_NB_result.append(1)\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_train_label,imdb_NB_result))\n",
    "print(\"The f1 score is: \",f1_score(imdb_train_label,imdb_NB_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_train_label,imdb_NB_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM,RF and LSTM,the training data and test data have to be further processed with Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train_dataset_vectorize=[]\n",
    "imdb_test_dataset_vectorize=[]\n",
    "for each_token_list in imdb_train_token_list_matrix:\n",
    "    vector = doc2vec_model.infer_vector(each_token_list)\n",
    "    imdb_train_dataset_vectorize.append(vector)\n",
    "for each_token_list in imdb_test_token_list_matrix:\n",
    "    vector = doc2vec_model.infer_vector(each_token_list)\n",
    "    imdb_test_dataset_vectorize.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_svm_model=SVC(C=1, kernel='rbf',random_state=10)\n",
    "imdb_svm_model.fit(imdb_train_dataset_vectorize,imdb_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of SVM on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.8841309823677582\n",
      "The f1 score is:  0.9036850921273032\n",
      "Confusion Matrix is:  [[ 676  123]\n",
      " [ 107 1079]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=imdb_svm_model.predict(imdb_test_dataset_vectorize)\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_test_label,predicted_result))\n",
    "print(\"The f1 score is: \",f1_score(imdb_test_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_test_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of SVM on Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.9349747474747475\n",
      "The f1 score is:  0.9457151892062822\n",
      "Confusion Matrix is:  [[2919  287]\n",
      " [ 228 4486]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=imdb_svm_model.predict(imdb_train_dataset_vectorize)\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_train_label,predicted_result))\n",
    "print(\"The f1 score is: \",f1_score(imdb_train_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_train_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=15, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=400,\n",
       "                       n_jobs=-1, oob_score=False, random_state=10, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_rf_model=RandomForestClassifier(n_estimators=400,criterion='gini',max_depth=15,random_state=10,n_jobs=-1)\n",
    "imdb_rf_model.fit(imdb_train_dataset_vectorize,imdb_train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of Random Forest on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.8327455919395466\n",
      "The f1 score is:  0.8684627575277337\n",
      "Confusion Matrix is:  [[ 557  242]\n",
      " [  90 1096]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=imdb_rf_model.predict(imdb_test_dataset_vectorize)\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_test_label,predicted_result))\n",
    "print(\"The f1 score is: \",f1_score(imdb_test_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_test_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of Random Forest on Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.9998737373737374\n",
      "The f1 score is:  0.9998939442146568\n",
      "Confusion Matrix is:  [[3205    1]\n",
      " [   0 4714]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=imdb_rf_model.predict(imdb_train_dataset_vectorize)\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_train_label,predicted_result))\n",
    "print(\"The f1 score is: \",f1_score(imdb_train_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_train_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Test Vader(Rule Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.7400503778337532\n",
      "Confusion Matrix is:  [[ 456    2  341]\n",
      " [   0    0    0]\n",
      " [ 170    3 1013]]\n"
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "vader_result=[]\n",
    "for each_tweet in imdb_test_raw_text:\n",
    "    score=sia.polarity_scores(each_tweet)['compound']\n",
    "    if(score<=-0.05):\n",
    "        vader_result.append(-1)\n",
    "    elif(score>0.05):\n",
    "        vader_result.append(1)\n",
    "    else:\n",
    "        vader_result.append(0)\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_test_label,vader_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_test_label,vader_result,labels=[-1,0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7920/7920 [==============================] - 1s 83us/step - loss: 0.0539 - accuracy: 0.8431\n",
      "Epoch 2/10\n",
      "7920/7920 [==============================] - 1s 76us/step - loss: 0.0389 - accuracy: 0.8912\n",
      "Epoch 3/10\n",
      "7920/7920 [==============================] - 1s 74us/step - loss: 0.0303 - accuracy: 0.9177\n",
      "Epoch 4/10\n",
      "7920/7920 [==============================] - 1s 73us/step - loss: 0.0215 - accuracy: 0.9453\n",
      "Epoch 5/10\n",
      "7920/7920 [==============================] - 1s 71us/step - loss: 0.0163 - accuracy: 0.9587\n",
      "Epoch 6/10\n",
      "7920/7920 [==============================] - 1s 73us/step - loss: 0.0134 - accuracy: 0.9667\n",
      "Epoch 7/10\n",
      "7920/7920 [==============================] - 1s 71us/step - loss: 0.0109 - accuracy: 0.9751\n",
      "Epoch 8/10\n",
      "7920/7920 [==============================] - 1s 73us/step - loss: 0.0092 - accuracy: 0.9785\n",
      "Epoch 9/10\n",
      "7920/7920 [==============================] - 1s 76us/step - loss: 0.0087 - accuracy: 0.9790\n",
      "Epoch 10/10\n",
      "7920/7920 [==============================] - 1s 75us/step - loss: 0.0089 - accuracy: 0.9794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1aabcf963c8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_nn_model = KerasClassifier(build_fn=neural_network_model, hidden_layer=3,\n",
    "                                        hidden_unit=256, opt='adam', epochs=10,\n",
    "                                        batch_size=32)\n",
    "imdb_nn_model.fit(np.array(imdb_train_dataset_vectorize),np.array(imdb_train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of Neural Network on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.8770780856423174\n",
      "The f1 score is:  0.8959044368600683\n",
      "Confusion Matrix is:  [[ 691  108]\n",
      " [ 136 1050]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=imdb_nn_model.predict(np.array(imdb_test_dataset_vectorize))\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_test_label,predicted_result))\n",
    "print(\"The f1 score is: \",f1_score(imdb_test_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_test_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the performance of Neural Network on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.9853535353535353\n",
      "The f1 score is:  0.9876857749469213\n",
      "Confusion Matrix is:  [[3152   54]\n",
      " [  62 4652]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=imdb_nn_model.predict(np.array(imdb_train_dataset_vectorize))\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_train_label,predicted_result))\n",
    "print(\"The f1 score is: \",f1_score(imdb_train_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_train_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use models trained on IMDb to Test Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.54635\n",
      "The f1 score is:  0.5815229924818965\n",
      "Confusion Matrix is:  [[4623 5389]\n",
      " [3684 6304]]\n"
     ]
    }
   ],
   "source": [
    "NB_result=[]\n",
    "for each_tweet in test_dataset_for_NB:\n",
    "    if IMDb_NB_classifier.classify(each_tweet)=='Negative':\n",
    "        NB_result.append(-1)\n",
    "    else:\n",
    "        NB_result.append(1)\n",
    "print(\"The accuracy score is: \",accuracy_score(test_label,NB_result))\n",
    "print(\"The f1 score is: \",f1_score(test_label,NB_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(test_label,NB_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.582\n",
      "The f1 score is:  0.5744247607411932\n",
      "Confusion Matrix is:  [[5998 4014]\n",
      " [4346 5642]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=imdb_svm_model.predict(test_dataset_vectorize)\n",
    "print(\"The accuracy score is: \",accuracy_score(test_label,predicted_result))\n",
    "print(\"The f1 score is: \",f1_score(test_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(test_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.547\n",
      "The f1 score is:  0.4497084548104956\n",
      "Confusion Matrix is:  [[7238 2774]\n",
      " [6286 3702]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=imdb_rf_model.predict(test_dataset_vectorize)\n",
    "print(\"The accuracy score is: \",accuracy_score(test_label,predicted_result))\n",
    "print(\"The f1 score is: \",f1_score(test_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(test_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.5643\n",
      "The f1 score is:  0.44882985452245416\n",
      "Confusion Matrix is:  [[7738 2274]\n",
      " [6440 3548]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=imdb_nn_model.predict(np.array(test_dataset_vectorize))\n",
    "print(\"The accuracy score is: \",accuracy_score(test_label,predicted_result))\n",
    "print(\"The f1 score is: \",f1_score(test_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(test_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use models trained on Twitter to Test IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.7234256926952141\n",
      "The f1 score is:  0.7658848614072497\n",
      "Confusion Matrix is:  [[538 261]\n",
      " [288 898]]\n"
     ]
    }
   ],
   "source": [
    "NB_result=[]\n",
    "for each_comment in imdb_test_dataset_for_NB:\n",
    "    if NB_classifier.classify(each_comment)=='Negative':\n",
    "        NB_result.append(-1)\n",
    "    else:\n",
    "        NB_result.append(1)\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_test_label,NB_result))\n",
    "print(\"The f1 score is: \",f1_score(imdb_test_label,NB_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_test_label,NB_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.4100755667506297\n",
      "The f1 score is:  0.04563977180114099\n",
      "Confusion Matrix is:  [[ 786   13]\n",
      " [1158   28]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=svm_model.predict(imdb_test_dataset_vectorize)\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_test_label,predicted_result))\n",
    "print(\"The f1 score is: \",f1_score(imdb_test_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_test_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.5717884130982368\n",
      "The f1 score is:  0.5319383259911895\n",
      "Confusion Matrix is:  [[652 147]\n",
      " [703 483]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=rf_model.predict(imdb_test_dataset_vectorize)\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_test_label,predicted_result))\n",
    "print(\"The f1 score is: \",f1_score(imdb_test_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_test_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is:  0.5768261964735516\n",
      "The f1 score is:  0.5945945945945945\n",
      "Confusion Matrix is:  [[529 270]\n",
      " [570 616]]\n"
     ]
    }
   ],
   "source": [
    "predicted_result=nn_model.predict(np.array(imdb_test_dataset_vectorize))\n",
    "print(\"The accuracy score is: \",accuracy_score(imdb_test_label,predicted_result))\n",
    "print(\"The f1 score is: \",f1_score(imdb_test_label,predicted_result))\n",
    "print(\"Confusion Matrix is: \",confusion_matrix(imdb_test_label,predicted_result,labels=[-1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
